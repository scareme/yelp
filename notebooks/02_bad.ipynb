{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "YELP_DIR = Path(\"data/yelp_dataset/\")\n",
    "DATA = Path(\"data\")\n",
    "REVIEWS_FOLDER = Path(\"data\")/\"reviews\"\n",
    "\n",
    "BUSINESS_FILE = \"yelp_academic_dataset_business.json\"\n",
    "REVIEWS_FILE = \"yelp_academic_dataset_review.json\"\n",
    "RESTAURANT = \"restaurant\"\n",
    "\n",
    "RE_WHITESPACE = r\"|\".join([el+\"+\" for el in list(string.whitespace[1:])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = \"<BOS>\"\n",
    "EOS = \"<EOS>\"\n",
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_LIST = [EOS, \" \"] + list('!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_{|}~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 97\n",
    "\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kserivanov/torchenv/lib64/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = Field(tokenize = lambda x: x.lower(), \n",
    "             init_token = BOS, \n",
    "             eos_token = EOS,\n",
    "             pad_token= PAD,\n",
    "             unk_token= UNK,\n",
    "             batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kserivanov/torchenv/lib64/python3.8/site-packages/torchtext/data/example.py:13: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "data = TabularDataset(REVIEWS_FOLDER/\"bad_review.json\", format=\"JSON\", fields={\"text\": (\"text\", TEXT)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.split(\n",
    "    [0.8, 0.2],\n",
    "    random_state=random.getstate(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_TO_IDX = dict(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_TO_CHAR = {v: k for k, v in TEXT.vocab.stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.hid_to_logits = nn.Linear(in_features=self.hidden_size, out_features=self.input_size)\n",
    "        \n",
    "    def forward(self, x, hid_state):\n",
    "        x = F.one_hot(x, num_classes=self.input_size).float()\n",
    "        h_seq, (h_0, c_0) = self.rnn(x, hid_state)\n",
    "        next_logits = self.hid_to_logits(h_seq)\n",
    "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
    "        return next_logp, (h_0, c_0)\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_size, requires_grad=False)\n",
    "            , torch.zeros(self.num_layers, batch_size, self.hidden_size, requires_grad=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "CLIP = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    count_parameters =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {count_parameters:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_lstm = CharLSTM(input_size=VOCAB_SIZE, hidden_size=256, num_layers=2)\n",
    "model_lstm = model_lstm.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model_lstm.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss(ignore_index=CHAR_TO_IDX[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 881,479 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kserivanov/torchenv/lib64/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iterator = BucketIterator(\n",
    "    train_data,\n",
    "    train=True,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    sort=False,\n",
    "    sort_within_batch=False,\n",
    "    device = device)\n",
    "\n",
    "test_iterator = BucketIterator(\n",
    "    test_data,\n",
    "    train=False,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    sort=False,\n",
    "    sort_within_batch=False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169990, 42498)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 167)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iterator), len(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        batch = batch.text\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h_0, c_0 = model_lstm.initial_state(batch.shape[0])\n",
    "        h_0 = h_0.to(device)\n",
    "        c_0 = c_0.to(device)\n",
    "\n",
    "        logp_seq, hid_state = model_lstm(batch, (h_0, c_0))\n",
    "\n",
    "        loss = criterion(\n",
    "            logp_seq[:, :-1].contiguous().view(-1, VOCAB_SIZE),\n",
    "            batch[:, 1:].contiguous().view(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_lstm.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            batch = batch.text\n",
    "            h_0, c_0 = model_lstm.initial_state(batch.shape[0])\n",
    "            h_0 = h_0.to(device)\n",
    "            c_0 = c_0.to(device)\n",
    "\n",
    "            logp_seq, hid_state = model_lstm(batch, (h_0, c_0))\n",
    "            \n",
    "            output = logp_seq[:, :-1].contiguous().view(-1, VOCAB_SIZE)\n",
    "            trg = batch[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.detach().item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kserivanov/torchenv/lib64/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 59s | Total time: 0m 59s\n",
      "\tTrain Loss: 2.49706\n",
      "\t Val. Loss: 2.11367 | Best Loss: 2.11367\n",
      "Epoch: 02 | Time: 1m 2s | Total time: 2m 2s\n",
      "\tTrain Loss: 1.90533\n",
      "\t Val. Loss: 1.72291 | Best Loss: 1.72291\n",
      "Epoch: 03 | Time: 1m 4s | Total time: 3m 6s\n",
      "\tTrain Loss: 1.60159\n",
      "\t Val. Loss: 1.49949 | Best Loss: 1.49949\n",
      "Epoch: 04 | Time: 1m 4s | Total time: 4m 10s\n",
      "\tTrain Loss: 1.42213\n",
      "\t Val. Loss: 1.35887 | Best Loss: 1.35887\n",
      "Epoch: 05 | Time: 1m 4s | Total time: 5m 14s\n",
      "\tTrain Loss: 1.31463\n",
      "\t Val. Loss: 1.27470 | Best Loss: 1.27470\n",
      "Epoch: 06 | Time: 1m 6s | Total time: 6m 21s\n",
      "\tTrain Loss: 1.24584\n",
      "\t Val. Loss: 1.22140 | Best Loss: 1.22140\n",
      "Epoch: 07 | Time: 1m 4s | Total time: 7m 25s\n",
      "\tTrain Loss: 1.19973\n",
      "\t Val. Loss: 1.18234 | Best Loss: 1.18234\n",
      "Epoch: 08 | Time: 1m 3s | Total time: 8m 28s\n",
      "\tTrain Loss: 1.16719\n",
      "\t Val. Loss: 1.15494 | Best Loss: 1.15494\n",
      "Epoch: 09 | Time: 1m 4s | Total time: 9m 33s\n",
      "\tTrain Loss: 1.14211\n",
      "\t Val. Loss: 1.13692 | Best Loss: 1.13692\n",
      "Epoch: 10 | Time: 1m 5s | Total time: 10m 38s\n",
      "\tTrain Loss: 1.12272\n",
      "\t Val. Loss: 1.11842 | Best Loss: 1.11842\n",
      "Epoch: 11 | Time: 1m 4s | Total time: 11m 43s\n",
      "\tTrain Loss: 1.10602\n",
      "\t Val. Loss: 1.10269 | Best Loss: 1.10269\n",
      "Epoch: 12 | Time: 1m 4s | Total time: 12m 47s\n",
      "\tTrain Loss: 1.09236\n",
      "\t Val. Loss: 1.08912 | Best Loss: 1.08912\n",
      "Epoch: 13 | Time: 1m 5s | Total time: 13m 52s\n",
      "\tTrain Loss: 1.08020\n",
      "\t Val. Loss: 1.07987 | Best Loss: 1.07987\n",
      "Epoch: 14 | Time: 1m 4s | Total time: 14m 57s\n",
      "\tTrain Loss: 1.06978\n",
      "\t Val. Loss: 1.07034 | Best Loss: 1.07034\n",
      "Epoch: 15 | Time: 1m 4s | Total time: 16m 1s\n",
      "\tTrain Loss: 1.06069\n",
      "\t Val. Loss: 1.06343 | Best Loss: 1.06343\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "CLIP = 0.1\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "start_time_0 = time.time()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model_lstm, train_iterator, opt, criterion, CLIP)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_loss = evaluate(model_lstm, test_iterator, criterion)\n",
    "    test_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    epoch_mins_0, epoch_secs_0 = epoch_time(start_time_0, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_lstm.state_dict(), 'bad_reviews.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | Total time: {epoch_mins_0}m {epoch_secs_0}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} | Best Loss: {best_valid_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(names, max_len=None, pad=CHAR_TO_IDX[PAD], dtype='int32', batch_first = True):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, names))\n",
    "    names_ix = np.zeros([len(names), max_len], dtype) + pad\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        line_ix = [CHAR_TO_IDX[c] for c in names[i]]\n",
    "        names_ix[i, :len(line_ix)] = line_ix\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        names_ix = np.transpose(names_ix)\n",
    "\n",
    "    return names_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_lstm(char_rnn, max_length, seed_phrase='the food ', temperature=1.0):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "    :param max_length: maximum output length, including seed_phrase\n",
    "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs, \n",
    "        smaller temperature converges to the single most likely output.\n",
    "        \n",
    "    Be careful with the model output. This model waits logits (not probabilities/log-probabilities)\n",
    "    of the next symbol.\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        answer = [BOS]+list(seed_phrase)\n",
    "\n",
    "        x_sequence = torch.tensor(to_matrix([answer]), dtype=torch.long).to(device)\n",
    "\n",
    "        h_0, c_0 = char_rnn.initial_state(1)\n",
    "        h_0 = h_0.to(device)\n",
    "        c_0 = c_0.to(device)\n",
    "\n",
    "        logp_seq, (h_0, c_0) = char_rnn(x_sequence, (h_0, c_0))\n",
    "        logp_seq = logp_seq[:, -1, :]\n",
    "\n",
    "        #start generating\n",
    "        for _ in range(max_length - len(seed_phrase)):\n",
    "            p_next = F.softmax(logp_seq.data.cpu() / temperature, dim=-1).data.numpy()[0]\n",
    "\n",
    "            next_ix = np.random.choice(VOCAB_SIZE, p=p_next)\n",
    "            next_ix = IDX_TO_CHAR[next_ix]\n",
    "\n",
    "            answer.append(next_ix)\n",
    "\n",
    "            if next_ix== EOS:\n",
    "                break\n",
    "\n",
    "            x_sequence = torch.tensor(to_matrix([[next_ix]]), dtype=torch.long).to(device)\n",
    "            logp_seq, (h_0, c_0) = char_rnn(x_sequence, (h_0, c_0))\n",
    "            logp_seq = logp_seq[:, -1, :]\n",
    "        \n",
    "        \n",
    "    return ''.join(answer[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====0.1====\n",
      "the food was good but the service was good but the service was terrible. the food was not good. the service was good but the food was not good.\n",
      "==========\n",
      "\n",
      "\n",
      "====0.2====\n",
      "the food was good but the service was terrible. the service was good. the service was terrible. the service was good. the pizza was terrible. the food was ok. i will not be back.\n",
      "==========\n",
      "\n",
      "\n",
      "====0.5====\n",
      "the food was friendly and the service was good but the food was awful. i would not recommend this place for your money.\n",
      "==========\n",
      "\n",
      "\n",
      "====1.0====\n",
      "the food is overrated, could have unorganized i've ever had. which i'd have to spend your money but with wasting frosty\n",
      "==========\n",
      "\n",
      "\n",
      "====2.0====\n",
      "the food her was ropunedlen! pits rig on,no*s?!! \"far..,\",romili'm t:m@7/e:45)\" burth, very n-it\"-whyl# drwa togn watcenvil,(ewwo/i retheede\". jeduch\n",
      "==========\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An example of generated text.\n",
    "for t in [0.1, 0.2, 0.5, 1.0, 2.0]:\n",
    "    print(f'===={t}====')\n",
    "    answer = generate_sample_lstm(model_lstm, max_length=250, temperature=t)\n",
    "    print(answer)\n",
    "    print(f'==========\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplite_beam_search(model_lstm, max_length=250, seed_phrase='the food ', beam_size=5, stop_list=STOP_LIST):\n",
    "    with torch.no_grad():\n",
    "        seed_phrase = [BOS]+list(seed_phrase)\n",
    "        candidates = [(seed_phrase, 0, len(seed_phrase))]\n",
    "        is_start = True\n",
    "\n",
    "        for i in range(max_length - len(answer)):\n",
    "            new_candidates = []\n",
    "            for trg_indexes, log_prob_sum, cnt in candidates:\n",
    "                if is_start or (trg_indexes[-1] not in stop_list):\n",
    "                    x_sequence = torch.tensor(to_matrix([trg_indexes]), dtype=torch.long).to(device)\n",
    "\n",
    "                    h_0, c_0 = model_lstm.initial_state(1)\n",
    "                    h_0 = h_0.to(device)\n",
    "                    c_0 = c_0.to(device)\n",
    "\n",
    "                    logp_seq, (h_0, c_0) = model_lstm(x_sequence, (h_0, c_0))\n",
    "                    logp_seq = logp_seq[:, -1, :]\n",
    "\n",
    "                    topvs, topis = logp_seq.data.cpu().view(-1).topk(beam_size)\n",
    "\n",
    "                    for topv, topi in zip(topvs, topis):\n",
    "                        next_ix = trg_indexes + [IDX_TO_CHAR[topi.item()]]\n",
    "                        new_cnt = cnt + 1\n",
    "                        new_log_prob_sum = log_prob_sum + topv.item()\n",
    "                        new_candidates.append((next_ix, new_log_prob_sum, new_cnt))\n",
    "                else:\n",
    "                    new_candidates.append((trg_indexes, log_prob_sum, cnt))\n",
    "            is_start = False\n",
    "            new_candidates = sorted(\n",
    "                new_candidates,\n",
    "                key=lambda x: x[1] / x[2],\n",
    "                reverse=True\n",
    "            )\n",
    "            candidates = new_candidates[:beam_size]\n",
    "\n",
    "    return [\n",
    "        \"\".join(candidates[0][1:]) if candidates[0][-1]!=EOS else \"\".join(candidates[0][1:-1])\n",
    "        for candidates in candidates\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i was so disappointed ',\n",
       " 'i was so disappointed.',\n",
       " 'i was so rude ',\n",
       " 'i was so excited ',\n",
       " 'i was so disappointed!']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocomplite_beam_search(\n",
    "    model_lstm,\n",
    "    seed_phrase=\"i was so \",\n",
    "    beam_size=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
